---
title: Feature Engineering with recipes
author: Aryan Kancherla and David Kane
tutorial:
  id: feature-engineering-with-recipes
output:
  learnr::tutorial:
    progressive: yes
    allow_skip: yes
runtime: shiny_prerendered
description: 'Tutorial for Chapter 8: Feature Engineering with recipes'
---

```{r setup, include = FALSE}
library(learnr)
library(tutorial.helpers)
library(tidyverse)
library(tidymodels)
tidymodels_prefer()

knitr::opts_chunk$set(echo = FALSE)
options(tutorial.exercise.timelimit = 60, 
        tutorial.storage = "local") 

ames_update <- ames |>
  mutate(Sale_Price = log10(Sale_Price))

set.seed(502)
ames_strata_split <- initial_split(ames_update, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_strata_split)
ames_test <- testing(ames_strata_split)

simple_ames <- recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type , data = ames_train) |>
  step_log(Gr_Liv_Area, base = 10) |>
    step_dummy(all_nominal_predictors())

lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_variables(outcome = Sale_Price, predictors = c(Longitude, Latitude))

lm_wflow_edit <- lm_wflow |>
  remove_variables() |>
    add_recipe(simple_ames)

lm_fit <- fit(lm_wflow_edit, ames_train)

neighborhood_plot <- ggplot(ames_train, aes(y = Neighborhood)) + 
  geom_bar() + 
  labs(y = NULL)

simple_ames1 <- recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type , data = ames_train) |>
  step_log(Gr_Liv_Area, base = 10) |>
    step_other(Neighborhood, theshold = 0.01) |>
      step_dummy(all_nominal_predictors())

facet_wrap_ames_plot <- ames_train |>
  ggplot(aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
  geom_point(alpha = 0.2) + 
  facet_wrap(~ Bldg_Type) + 
  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = "lightblue") + 
  scale_x_log10() + 
  scale_y_log10() + 
  labs(x = "Gross Living Area", y = "Sale Price (USD)")

```

```{r copy-code-chunk, child = system.file("child_documents/copy_button.Rmd", package = "tutorial.helpers")}
```

```{r info-section, child = system.file("child_documents/info_section.Rmd", package = "tutorial.helpers")}
```

<!-- Note to self: Use section 8.3 for knowledge drops. -->
<!-- Note to self: Finish "Introduction" Section -->

## Introduction
### 

This tutorial covers [Chapter 8: Feature Engineering with recipies](https://www.tmwr.org/recipes.html) from [*Tidy Modeling with R*](https://www.tmwr.org/) by Max Kuhn and Julia Silge. This tutorial covers the usage of `recipe()`.


## A simple `recipe()` for the Ames housing data
### 

In this section, we will focus on a small subset of the predictors available in the `ames` housing data:

- The neighborhood (qualitative, with 29 neighborhoods in the training set)
- The gross above-grade living area (continuous, named `Gr_Liv_Area`)
- The year built (`Year_Built`)
- The type of building (`Bldg_Type` with values `OneFam` (n = 1936), `TwoFmCon` (n = 50), `Duplex` (n = 88), `Twnhs` (n = 77), `TwnhsE` (n = 191)

### Exercise 1

Load the **tidymodels** library using `library()`.

```{r a-simple-recipe-for--1, exercise = TRUE}

```

```{r a-simple-recipe-for--1-hint-1, eval = FALSE}
library(...)
```

```{r include = FALSE}
library(tidymodels)
```

### 

Feature engineering entails reformatting predictor values to make them easier for a model to use effectively. This includes transformations and encodings of the data to best represent their important characteristics. Imagine that you have two predictors in a data set that can be more effectively represented in your model as a ratio; creating a new predictor from the ratio of the original two is a simple example of feature engineering.

### Exercise 2

Type in `tidymodels_prefer()` to get rid of naming conflicts.

```{r a-simple-recipe-for--2, exercise = TRUE}

```

```{r a-simple-recipe-for--2-hint-1, eval = FALSE}
...()
```

```{r include = FALSE}
tidymodels_prefer()
```

### 

A recipe is also an object that defines a series of steps for data processing. Unlike the formula method inside a modeling function, the recipe defines the steps via `step_*()` functions without immediately executing them; it is only a specification of what should be done.

### Exercise 3

Type in `?recipe()` in the Console and look at the Description section. CP/CR.

```{r a-simple-recipe-for--3}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

For example, when choosing how to encode the location of a house in Ames in modeling, we might choose an option we believe is most associated with the outcome. The original format of the data, for example numeric (e.g., distance) versus categorical (e.g., neighborhood), is also a driving factor in feature engineering choices. This is called *preprocessing*.

### Exercise 4

Type in `ames_train` and press "Run Code".

```{r a-simple-recipe-for--4, exercise = TRUE}

```

```{r a-simple-recipe-for--4-hint-1, eval = FALSE}
...
```

```{r include = FALSE}
ames_train
```

### 

This variable was created in [Chapter 5: Spending our Data](https://www.tmwr.org/splitting.html). As a reminder, this variable contains the training data from the `ames` data set (which has been pre-logged). 

### Exercise 5

Lets create a recipe using `ames_train`. Type in `recipe()` and set `formula` to `Sale_Price ~ Neighborhood`. Then, set `data` to `ames_train`.

```{r a-simple-recipe-for--5, exercise = TRUE}

```

```{r a-simple-recipe-for--5-hint-1, eval = FALSE}
...(... = Sale_Price ~ Neighborhood, data = ...)
```

```{r include = FALSE}
recipe(formula = Sale_Price ~ Neighborhood, data = ames_train)
```

### 

Other examples of preprocessing to build better features for modeling include:

- Correlation between predictors can be reduced via feature extraction or the removal of some predictors.

- When some predictors have missing values, they can be imputed using a sub-model.

- Models that use variance-type measures may benefit from coercing the distribution of some skewed predictors to be symmetric by estimating a transformation.

### Exercise 6

Copy the previous code. After `Neighborhood`, add `Gr_Liv_Area`, `Year_Built`, and `Bldg_Type` using `+`.

```{r a-simple-recipe-for--6, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r a-simple-recipe-for--6-hint-1, eval = FALSE}
recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + ... + ... , data = ames_train)

```

```{r include = FALSE}
recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type , data = ames_train)
```

### 

The call to `recipe()` with a formula tells the recipe the *roles* of the “ingredients” or variables (e.g., predictor, outcome). It only uses the data `ames_train` to determine the data types for the columns.

### Exercise 7

Next, the `step_log()` function is going to be used. In the Console, type in `?step_log()` and look at the *Description* section. CP/CR.

```{r a-simple-recipe-for--7}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

Feature engineering and data preprocessing can also involve reformatting that may be required by the model. Some models use geometric distance metrics and, consequently, numeric predictors should be centered and scaled so that they are all in the same units. Otherwise, the distance values would be biased by the scale of each column.


### Exercise 8

Copy the code from Exercise 6 and pipe it to `step_log()`. Inside this function, type in `Gr_Liv_Area` and set `base` to `10`.

```{r a-simple-recipe-for--8, exercise = TRUE}

```

```{r a-simple-recipe-for--8-hint-1, eval = FALSE}
recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type , data = ames_train) |>
  step_log(..., base = ...)
```

```{r include = FALSE}
recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type , data = ames_train) |>
  step_log(Gr_Liv_Area, base = 10)
```

### 

`step_log()` declares that `Gr_Liv_Area` should be log transformed.

### Exercise 9

Next, the `step_dummy()` function will be used. In the Console, type `?step_dummy()` and look at the *Description* section. CP/CR.

```{r a-simple-recipe-for--9}
question_text(NULL,
	answer(NULL, correct = TRUE),
	allow_retry = TRUE,
	try_again_button = "Edit Answer",
	incorrect = NULL,
	rows = 3)
```

### 

Different models have different preprocessing requirements and some, such as tree-based models, require very little preprocessing at all.


### Exercise 10

Copy the code from Exercise 8 and pipe it to `step_dummy()`. Inside of the function, type in `all_nominal_predictors()`.  

```{r a-simple-recipe-for--10, exercise = TRUE}

```

```{r a-simple-recipe-for--10-hint-1, eval = FALSE}
recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type , data = ames_train) |>
  step_log(Gr_Liv_Area, base = 10) |>
    step_dummy(...())
```

```{r include = FALSE}
recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type , data = ames_train) |>
  step_log(Gr_Liv_Area, base = 10) |>
    step_dummy(all_nominal_predictors())
```

### 

`step_dummy()` specifies which variables should be converted from a qualitative format to a quantitative format, in this case, using dummy or indicator variables. An indicator or dummy variable is a binary numeric variable (a column of ones and zeroes) that encodes qualitative information

The function `all_nominal_predictors()` captures the names of any predictor columns that are currently factor or character (i.e., nominal) in nature. This is a **dplyr**-like selector function similar to `starts_with()` or `matches()` but that can only be used inside of a recipe

### Exercise 11

Copy the previous code and assign it to a new variable called `simple_ames`. 

```{r a-simple-recipe-for--11, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r a-simple-recipe-for--11-hint-1, eval = FALSE}
... <- recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type , data = ames_train) |>
  step_log(Gr_Liv_Area, base = 10) |>
    step_dummy(all_nominal_predictors())
```

```{r include = FALSE}
simple_ames <- recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type , data = ames_train) |>
  step_log(Gr_Liv_Area, base = 10) |>
    step_dummy(all_nominal_predictors())
```

### 

What is the advantage to using a recipe, over a formula or raw predictors? There are a few, including:

- These computations can be recycled across models since they are not tightly coupled to the modeling function.

- A recipe enables a broader set of data processing choices than formulas can offer.

- The syntax can be very compact. For example, all_nominal_predictors() can be used to capture many variables for specific types of processing while a formula would require each to be explicitly listed.

- All data processing can be captured in a single R object instead of in scripts that are repeated, or even spread across different files.

### 

Congrats! You have learned how to create a recipe.

## Using Recipes
### 

Preprocessing choices and feature engineering should typically be considered part of a modeling workflow, not a separate task. The **workflows** package contains high level functions to handle different types of preprocessors. The previous workflow (`lm_wflow`) used a simple set of dplyr selectors. To improve on that approach with more complex feature engineering, let’s use the `simple_ames` recipe to preprocess data for modeling.

### Exercise 1

Type in `lm_wflow` and press "Run Code".

```{r using-recipes-1, exercise = TRUE}

```

```{r using-recipes-1-hint-1, eval = FALSE}
...
```

```{r include = FALSE}
lm_wflow
```

### 

As a reminder, a *model workflow* encapsulates the major pieces of the modeling process by encouraging good methodology (since it is a single point of entry to the estimation components of a data analysis) and enabling the user to better organize projects.

Here is the code for `lm_wflow`, which was created in the *A Model Workflow* tutorial:

````
lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_variables(outcome = Sale_Price, predictors = c(Longitude, Latitude))
  
````

### Exercise 2

Lets attach `simple_ames` to this workflow. In the code chunk below, pipe `lm_wflow` to `add_recipe()`. Inside this function, pass in `simple_ames` (Note: This will throw an error).

```{r using-recipes-2, exercise = TRUE}

```

```{r using-recipes-2-hint-1, eval = FALSE}
lm_wflow |>
  add_recipe(...)
```

```{r include = FALSE}
# lm_wflow |>
#  add_recipe(simple_ames)
```

### 

There can only be one preprocessing method at a time, so the existing preprocessor needs to be removed before adding the recipe.

### Exercise 3

Copy the previous code. Instead of piping `lm_wflow` to `add_recipe()`, pipe `lm_wflow` to `remove_variables()` and then create the pipe to `add_recipe()`.

```{r using-recipes-3, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r using-recipes-3-hint-1, eval = FALSE}
lm_wflow |>
  ...() |>
    add_recipe(simple_ames)
```

```{r include = FALSE}
lm_wflow |>
  remove_variables() |>
    add_recipe(simple_ames)
```

### 

Take a look at the output. The *Workflow* section shows that this model is a linear regression model, with a preprocessor of `recipe`. The *Preprocessor* section shows the steps that have been created: `step_log()`, which declared that `Gr_Liv_Area` should be log transformed, and `step_dummy()`, which specified which variables should be converted from a qualitative format to a quantitative format.

### Exercise 4

Copy the previous code and assign it to `lm_wflow_edit`. 

```{r using-recipes-4, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r using-recipes-4-hint-1, eval = FALSE}
... <- lm_wflow |>
  remove_variables() |>
    add_recipe(simple_ames)
```

```{r include = FALSE}
lm_wflow_edit <- lm_wflow |>
  remove_variables() |>
    add_recipe(simple_ames)
```

### 

Other selectors specific to the recipes package are: `all_numeric_predictors()`, `all_numeric()`, `all_predictors()`, and `all_outcomes()`. As with **dplyr**, one or more unquoted expressions, separated by commas, can be used to select which columns are affected by each step.

### Exercise 5

Now, let’s estimate both the recipe and model using a simple call to `fit()`.Type in `fit()`, passing in `lm_wflow_edit` and `ames_train`. 

```{r using-recipes-5, exercise = TRUE}

```

```{r using-recipes-5-hint-1, eval = FALSE}
fit(..., ames_train)
```

```{r include = FALSE}
fit(lm_wflow_edit, ames_train)
```

### 

<!-- AK: Add knowledge drop -->

### Exercise 6

Copy the previous code and assign it to a new variable called `lm_fit`.

```{r using-recipes-6, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r using-recipes-6-hint-1, eval = FALSE}
... <- fit(lm_wflow_edit, ames_train)
```

```{r include = FALSE}
lm_fit <- fit(lm_wflow_edit, ames_train)
```

### 

Take a look at the following code:

````
lm(Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Year_Built + Bldg_Type, data = ames)
````

When this function is executed, the data are converted from a data frame to a numeric design matrix (also called a model matrix) and then the least squares method is used to estimate parameters.

### Exercise 7

The `predict()` method applies the same preprocessing that was used on the training set to the new data before passing them along to the model’s `predict()` method.

Type in `predict()`, passing in `lm_fit` and `ames_train`.

<!-- AK: Getting a warning message here and unsure of how to fix it.  -->

```{r using-recipes-7, exercise = TRUE}

```

```{r using-recipes-7-hint-1, eval = FALSE}
predict(..., ...)
```

```{r include = FALSE}
predict(lm_fit, ames_test)
```

### 

This [link](https://www.tmwr.org/pre-proc-table.html#pre-proc-table) contains a small table of recommended preprocessing techniques for different models.

### Exercise 8

The `extract_recipe()` function can be used to retrieve a bare model object or recipe.

In the code chunk below, pipe `lm_fit` to `extract_recipe()`. Inside this function, set `estimated` to `TRUE`.

```{r using-recipes-8, exercise = TRUE}

```

```{r using-recipes-8-hint-1, eval = FALSE}
lm_fit |>
  ...(estimated = ...)
```

```{r include = FALSE}
lm_fit |>
  extract_recipe(estimated = TRUE)
```

### 

As you can see, this code produces all the information about the recipe that was created at the beginning of this section, including the inputs, training data information, and the operations on the recipe.

### Exercise 9

Finally, lets tidy the model fit, ultimately producing a tibble. Start by piping `lm_fit` to `extract_fit_parsnip()`.

```{r using-recipes-9, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r using-recipes-9-hint-1, eval = FALSE}
lm_fit |>
  ...()
```

```{r include = FALSE}
lm_fit |>
  extract_fit_parsnip()
```

### 

`extract_fit_parsnip()` returns the parsnip object, including the coefficients and the function that was used to create the model, which in this case is `lm()`.

### Exercise 10

Copy the previous code and pipe it to `tidy()`. Then, pipe the code to `slice(1:5)`. 

```{r using-recipes-10, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r using-recipes-10-hint-1, eval = FALSE}

```

```{r include = FALSE}
lm_fit |>
  extract_fit_parsnip() |>
    tidy() |>
      slice(1:5)
```

### 

- `estimate` stands for the estimated coefficient for each predictor variable, representing its effect on the outcome variable.

- std.error` stands for the standard error associated with the estimate, representing the uncertainty or variability in the coefficient estimate.

- `statistic` stands for the t-statistic or z-statistic, which is used to test the null hypothesis that the corresponding coefficient is equal to zero.

- `p-value` stands for the p-value associated with the statistical test for each predictor variable. It indicates the probability of observing the data if the null hypothesis (the coefficient is zero) is true. Smaller p-values suggest stronger evidence against the null hypothesis.

### 

Congrats! You have learned how to use recipes.

## Examples of Recipe Steps
### 

Before proceeding, let’s take an extended tour of the capabilities of recipes and explore some of the most important `step_*()` functions. These recipe step functions each specify a specific possible step in a feature engineering process, and different recipe steps can have different effects on columns of data.

### Exercise 1

Load the **tidyverse** library using `library()`.

```{r examples-of-recipe-s-1, exercise = TRUE}

```

```{r examples-of-recipe-s-1-hint-1, eval = FALSE}
library(...)
```

```{r include = FALSE}
library(tidyverse)
```

### 

One of the most common feature engineering tasks is transforming nominal or qualitative data (factors or characters) so that they can be encoded or represented numerically. Sometimes the factor levels of a qualitative column can be altered in helpful ways prior to such a transformation. For example, `step_unknown()` can be used to change missing values to a dedicated factor level. 

### Exercise 2

Lets create the following plot:

```{r}
neighborhood_plot
```

Start by piping `ames_train` to `ggplot()`.


```{r examples-of-recipe-s-2, exercise = TRUE}

```

```{r examples-of-recipe-s-2-hint-1, eval = FALSE}
ames_train |>
  ...()
```

```{r include = FALSE}
ames_train |>
  ggplot()
```

### 

Anticipating that a new factor level may be encountered in future data, the `step_novel()` function can allot a new level. 

### Exercise 3

Copy the previous code. Inside `ggplot()`, type in `aes()` and set `y` to `Neighborhood`.

```{r examples-of-recipe-s-3, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r examples-of-recipe-s-3-hint-1, eval = FALSE}
ames_train |>
  ggplot(aes(... = ...))
```

```{r include = FALSE}
ames_train |>
  ggplot(aes(y = Neighborhood))
```

### 

Additionally, `step_other()` can be used to analyze the frequencies of the factor levels in the training set and convert infrequently occurring values to a catch-all level of “other,” with a threshold that can be specified.

### Exercise 4

Copy the previous code and add `geom_bar()`.

```{r examples-of-recipe-s-4, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r examples-of-recipe-s-4-hint-1, eval = FALSE}
ames_train |>
  ggplot(aes(y = Neighborhood)) +
    ...()
```

```{r include = FALSE}
ames_train |>
  ggplot(aes(y = Neighborhood)) +
    geom_bar()
```

### 

This graph shows that two neighborhoods have less than five properties in the training data (Landmark and Green Hills); in this case, no houses at all in the Landmark neighborhood were included in the testing set. For some models, it may be problematic to have dummy variables with a single nonzero entry in the column. At a minimum, it is highly improbable that these features would be important to a model.

### Exercise 5

If `step_other(Neighborhood, threshold = 0.01)` is added to the `simple_ames` recipe, the bottom 1% of the neighborhoods will be lumped into a new level called “other.” In this training set, this will catch seven neighborhoods.

Take a look at `simple_ames`, which you created earlier in this tutorial. After `step_log()`, but before `step_dummy()`, create a pipe to `step_other()`. Inside this function, type in `Neighborhood` and set `threshold` to `0.01`.

```{r examples-of-recipe-s-5, exercise = TRUE}
recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type , data = ames_train) |>
  step_log(Gr_Liv_Area, base = 10) |>
    step_dummy(all_nominal_predictors())
```

```{r examples-of-recipe-s-5-hint-1, eval = FALSE}
recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type , data = ames_train) |>
  step_log(Gr_Liv_Area, base = 10) |>
    step_other(..., theshold = ...) |>
      step_dummy(all_nominal_predictors())
```

```{r include = FALSE}
recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type , data = ames_train) |>
  step_log(Gr_Liv_Area, base = 10) |>
    step_other(Neighborhood, theshold = 0.01) |>
      step_dummy(all_nominal_predictors())
```

### 

Many, but not all, underlying model calculations require predictor values to be encoded as numbers. Notable exceptions include tree-based models, rule-based models, and naive Bayes models.

### Exercise 6

Copy the previous code and assign it to a new variable called `simple_ames1`.


```{r examples-of-recipe-s-6, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r examples-of-recipe-s-6-hint-1, eval = FALSE}
... <- recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type , data = ames_train) |>
  step_log(Gr_Liv_Area, base = 10) |>
    step_other(Neighborhood, theshold = 0.01) |>
      step_dummy(all_nominal_predictors())
```

```{r include = FALSE}
simple_ames1 <- recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type , data = ames_train) |>
  step_log(Gr_Liv_Area, base = 10) |>
    step_other(Neighborhood, theshold = 0.01) |>
      step_dummy(all_nominal_predictors())
```

### 

All preprocessing and feature engineering steps use only the training data. Otherwise, information leakage can negatively impact the model’s performance when used with new data.

### Exercise 7

Interaction effects involve two or more predictors. Such an effect occurs when one predictor has an effect on the outcome that is contingent on one or more other predictors. Numerically, an interaction term between predictors is encoded as their product. Interactions are defined in terms of their effect on the outcome and can be combinations of different types of data (e.g., numeric, categorical, etc). 

Lets see whether the regression slopes for the gross living area differ for different building types in the `ames` data set. Start by piping `ames_train` to `ggplot()`

```{r examples-of-recipe-s-7, exercise = TRUE}

```

```{r examples-of-recipe-s-7-hint-1, eval = FALSE}
ames_train |>
  ...()
```

```{r include = FALSE}
ames_train |>
  ggplot()
```

### 

Here's an example of interaction effects: if you were trying to predict how much traffic there will be during your commute, two potential predictors could be the specific time of day you commute and the weather. However, the relationship between the amount of traffic and bad weather is different for different times of day. In this case, you could add an interaction term between the two predictors to the model along with the original two predictors (which are called the main effects).

### Exercise 8

Copy the previous code. Inside `ggplot()`, using `aes()`, set `x` to `Gr_Liv_Area` and `y` to `10^Sale_Price`.

```{r examples-of-recipe-s-8, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r examples-of-recipe-s-8-hint-1, eval = FALSE}
ames_train |>
  ggplot(aes(x = ..., y = ...))
```

```{r include = FALSE}
ames_train |>
  ggplot(aes(x = Gr_Liv_Area, y = 10^Sale_Price))
```

### 

One helpful feature of `step_dummy()` is that there is more control over how the resulting dummy variables are named. In base R, dummy variable names mash the variable name with the level, resulting in names like `NeighborhoodVeenker`.

### Exercise 9

Copy the previous code and add `geom_point()`. Inside this function set `alpha` to `0.2`.

```{r examples-of-recipe-s-9, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r examples-of-recipe-s-9-hint-1, eval = FALSE}
ames_train |>
  ggplot(aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
  geom_point(... = ...)
```

```{r include = FALSE}
ames_train |>
  ggplot(aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
  geom_point(alpha = 0.2)
```

### 

Recipes, by default, use an underscore as the separator between the name and level (e.g., `Neighborhood_Veenker`) and there is an option to use custom formatting for the names. The default naming convention in recipes makes it easier to capture those new columns in future steps using a selector, such as `starts_with("Neighborhood_")`.

### Exercise 10

Copy the previous code and add `facet_wrap()`, passing in `~ Bldg_Type`.

```{r examples-of-recipe-s-10, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r examples-of-recipe-s-10-hint-1, eval = FALSE}
ames_train |>
  ggplot(aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
  geom_point(alpha = 0.2) + 
  facet_wrap(...)
```

```{r include = FALSE}
ames_train |>
  ggplot(aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
  geom_point(alpha = 0.2) + 
  facet_wrap(~ Bldg_Type)
```

### 

`facet_wrap()` is a way of visualizing data by dividing it into subsets based on one or more categorical variables and creating separate panels or plots for each subset. As you can see, the graph is split into five different graphs, which have been separated by each `BLdg_Type`.

### Exercise 11

Copy the previous code and add `geom_smooth()`. Inside this function, set `method` to `lm` and `formula` to `y ~ x`.

```{r examples-of-recipe-s-11, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r examples-of-recipe-s-11-hint-1, eval = FALSE}
ames_train |>
  ggplot(aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
  geom_point(alpha = 0.2) + 
  facet_wrap(~ Bldg_Type) + 
  geom_smooth(... = lm, ... = y ~ x)
```

```{r include = FALSE}
ames_train |>
  ggplot(aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
  geom_point(alpha = 0.2) + 
  facet_wrap(~ Bldg_Type) + 
  geom_smooth(method = lm, formula = y ~ x)
```

### 

Different recipe steps behave differently when applied to variables in the data. For example, `step_log()` modifies a column in place without changing the name. Other steps, such as `step_dummy()`, eliminate the original data column and replace it with one or more columns with different names. The effect of a recipe step depends on the type of feature engineering transformation being done.


### Exercise 12

Copy the previous code. Inside `geom_smooth()`, set `se` to `FALSE` and `color` to `"lightblue"`.

```{r examples-of-recipe-s-12, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r examples-of-recipe-s-12-hint-1, eval = FALSE}
ames_train |>
  ggplot(aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
  geom_point(alpha = 0.2) + 
    facet_wrap(~ Bldg_Type) + 
    geom_smooth(method = lm, formula = y ~ x, se = ..., color = "...")
```

```{r include = FALSE}
ames_train |>
  ggplot(aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
  geom_point(alpha = 0.2) + 
  facet_wrap(~ Bldg_Type) + 
  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = "lightblue")
```

### 

Traditional dummy variables require that all of the possible categories be known to create a full set of numeric features. There are other methods for doing this transformation to a numeric format:

*Feature hashing* methods only consider the value of the category to assign it to a predefined pool of dummy variables. 

*Effect or likelihood* encodings replace the original data with a single numeric column that measures the effect of those data. 

Both feature hashing and effect encoding can seamlessly handle situations where a novel factor level is encountered in the data.

### Exercise 13

Copy the previous code and add `scale_x_log10()`.

```{r examples-of-recipe-s-13, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r examples-of-recipe-s-13-hint-1, eval = FALSE}
ames_train |>
  ggplot(aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
  geom_point(alpha = 0.2) + 
  facet_wrap(~ Bldg_Type) + 
  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = "lightblue") + 
  ...()
```

```{r include = FALSE}
ames_train |>
  ggplot(aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
  geom_point(alpha = 0.2) + 
  facet_wrap(~ Bldg_Type) + 
  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = "lightblue") + 
  scale_x_log10()
```

### 

Data are passed to recipes at different stages:

First, when calling `recipe(..., data)`, the data set is used to determine the data types of each column so that selectors such as `all_numeric()` or `all_numeric_predictors()` can be used.

Second, when preparing the data using `fit(workflow, data)`, the training data are used for all estimation operations including a recipe that may be part of the `workflow`, from determining factor levels to computing PCA components and everything in between.

### Exercise 14

Copy the previous code and add `scale_y_log10()`.

```{r examples-of-recipe-s-14, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r examples-of-recipe-s-14-hint-1, eval = FALSE}
ames_train |>
  ggplot(aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
  geom_point(alpha = 0.2) + 
  facet_wrap(~ Bldg_Type) + 
  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = "lightblue") + 
  scale_x_log10() + 
  ...()
```

```{r include = FALSE}
ames_train |>
  ggplot(aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
  geom_point(alpha = 0.2) + 
  facet_wrap(~ Bldg_Type) + 
  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = "lightblue") + 
  scale_x_log10() + 
  scale_y_log10()
```

### 

All preprocessing and feature engineering steps use only the training data. Otherwise, information leakage can negatively impact the model’s performance when used with new data.

### Exercise 15

Finally, copy the previous code and `labs()`. The final graph should look like this:

```{r}
facet_wrap_ames_plot
```

```{r examples-of-recipe-s-15, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r examples-of-recipe-s-15-hint-1, eval = FALSE}
ames_train |>
  ggplot(aes(x = Gr_Liv_Area, y = 10^Sale_Price)) +
  geom_point(alpha = 0.2) + 
  facet_wrap(~ Bldg_Type) + 
  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = "lightblue") + 
  scale_x_log10() + 
  scale_y_log10() + 
  labs(x = "...", y = "...")
```

```{r include = FALSE}
facet_wrap_ames_plot
```

### 

Going back to the purpose of creating this graph, you can see that the regression slopes for the gross living area do in fact differ for different building types.

## Summary
### 

<!-- Two to four sentences which bring the lessons of the tutorial together for the student. What do they know now that they did not know before? How does this tutorial connect to other tutorials? OK if this is very similar to the Introduction. You made a promise as to what they would learn. You (we hope!) kept that promise.-->

```{r download-answers, child = system.file("child_documents/download_answers.Rmd", package = "tutorial.helpers")}
```
