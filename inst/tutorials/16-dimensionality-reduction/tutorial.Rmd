---
title: Dimensionality Reduction
author: Aryan Kancherla
tutorial:
  id: dimensionality-reduction
output:
  learnr::tutorial:
    progressive: yes
    allow_skip: yes
runtime: shiny_prerendered
description: 'Tutorial for Chapter 16: Dimensionality Reduction'
---

```{r setup, include = FALSE}
library(learnr)
library(tutorial.helpers)
library(knitr)

library(tidymodels)
library(beans)
library(corrplot)
library(bestNormalize)
library(ggforce)
library(learntidymodels)
tidymodels_prefer()

knitr::opts_chunk$set(echo = FALSE)
options(tutorial.exercise.timelimit = 60, 
        tutorial.storage = "local") 

set.seed(1601)

bean_split <- initial_split(beans, strata = class, prop = 3/4)
bean_train <- training(bean_split)
bean_test <- testing(bean_split)

bean_val <- validation_split(bean_train, strata = class, prop = 3/4)

tmwr_cols <- colorRampPalette(c("#91CBD765", "#CA225E"))

bean_rec <- 
  recipe(class ~ ., data = analysis(bean_val$splits[[1]])) |>
  step_zv(all_numeric_predictors()) |>
  step_orderNorm(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors())

bean_rec_trained <- prep(bean_rec)

bean_validation <- 
  bean_val$splits |>
  pluck(1) |>
  assessment()

bean_val_processed <- bake(bean_rec_trained, new_data = bean_validation)


p1 <- 
  bean_validation |> 
  ggplot(aes(x = area)) +
  geom_histogram(bins = 30, alpha = 1/3, color = "white", fill = "blue") +
  ggtitle("Original validation set data")

p2 <-
  bean_val_processed |> 
  ggplot(aes(x = area)) + 
  geom_histogram(bins = 30, color = "white", fill = "red", alpha = 1/3) + 
  ggtitle("Processed validation set data")

plot_validation_results <- function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    bake(new_data = dat) |>
    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +
    geom_point(alpha = 0.4, size = 0.5) +
    geom_autodensity(alpha = 0.3) +
    facet_matrix(vars(-class), layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2") + 
    scale_fill_brewer(palette = "Dark2")
}


```

```{r copy-code-chunk, child = system.file("child_documents/copy_button.Rmd", package = "tutorial.helpers")}
```

```{r info-section, child = system.file("child_documents/info_section.Rmd", package = "tutorial.helpers")}
```

## Introduction
### 

<!-- Two to four sentence about the main topics covered in this tutorial. Why are we here? What will students get out of giving you 90 minutes of their lives? How does this tutorial connect to other tutorials? -->

## A Picture Is Worth A Thousand...Beans
### 

Dimensionality reduction transforms a data set from a high-dimensional space into a low-dimensional space, and can be a good choice when you suspect there are “too many” variables. An excess of variables, usually predictors, can be a problem because it is difficult to understand or visualize data in higher dimensions.

### Exercise 1

Load the **tidymodels** library using `library()`.

```{r a-picture-is-worth-a-1, exercise = TRUE}

```

```{r a-picture-is-worth-a-1-hint-1, eval = FALSE}
library(...)
```

```{r include = FALSE}
library(tidymodels)
```

### 

Let’s walk through how to use dimensionality reduction with recipes for an example data set. Murat Koklu, an Academician at Selcuk University, and İlker Ali ÖZKAN, a Faculty of Technology at Selcuk University, published a data set of visual characteristics of dried beans and described methods for determining the varieties of dried beans in an image. While the dimensionality of these data is not very large compared to many real-world modeling problems, it does provide a nice working example to demonstrate how to reduce the number of features.

### Exercise 2

Type in `tidymodels_prefer()` to get rid of naming conflicts.

```{r a-picture-is-worth-a-2, exercise = TRUE}

```

```{r a-picture-is-worth-a-2-hint-1, eval = FALSE}
...()
```

```{r include = FALSE}
tidymodels_prefer()
```

### 

This is an excerpt from Koklu's and ÖZKAN's manuscript:

"The primary objective of this study is to provide a method for obtaining uniform seed varieties from crop production, which is in the form of population, so the seeds are not certified as a sole variety. Thus, a computer vision system was developed to distinguish seven different registered varieties of dry beans with similar features in order to obtain uniform seed classification. For the classification model, images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera".

### Exercise 3

The data created by Koklu and ÖZKAN will be used. Load the **beans** package using `library()`. 

```{r a-picture-is-worth-a-3, exercise = TRUE}

```

```{r a-picture-is-worth-a-3-hint-1, eval = FALSE}
library(...)
```

```{r include = FALSE}
library(beans)
```

### 

Each image in the data contains multiple beans. The process of determining which pixels correspond to a particular bean is called *image segmentation*. These pixels can be analyzed to produce features for each bean, such as color and morphology (i.e., shape). These features are then used to model the outcome (bean variety) because different bean varieties look different.

The training data come from a set of manually labeled images, and this data set is used to create a predictive model that can distinguish between seven bean varieties: Cali, Horoz, Dermason, Seker, Bombay, Barbunya, and Sira. Producing an effective model can help manufacturers quantify the homogeneity of a batch of beans.

### Exercise 4

Lets take a look at the `beans` data set. In the code chunk below, type in `beans` and press "Run code".

```{r a-picture-is-worth-a-4, exercise = TRUE}

```

```{r a-picture-is-worth-a-4-hint-1, eval = FALSE}
beans
```

```{r include = FALSE}
beans
```

### 

As you can see, this data set of 58 beans contains various details about each bean, including the `area`, `compactness`, and `aspect_ratio`. 

### Exercise 5

Type in `set.seed()` and pass in `1601`.

```{r a-picture-is-worth-a-5, exercise = TRUE}

```

```{r a-picture-is-worth-a-5-hint-1, eval = FALSE}
set.seed(...)
```

```{r include = FALSE}
set.seed(1601)
```

### 

There are numerous methods for quantifying shapes of objects. Many are related to the boundaries or regions of the object of interest. One feature is the area: the area (or size) can be estimated using the number of pixels in the object or the size of the convex hull around the object.

### Exercise 6

Now, lets split the data and create a training and testing set. In the code chunk below, type in `initial_split()`. Inside this function, type in `beans` and set `strata` to `class`.

```{r a-picture-is-worth-a-6, exercise = TRUE}

```

```{r a-picture-is-worth-a-6-hint-1, eval = FALSE}
initial_split(..., strata = ...)
```

```{r include = FALSE}
initial_split(beans, strata = class)
```

### 

As you can see, this code splits the data into a training and testing set, with 13611 total values. However, the desired split should be 75% training and 25 testing, which is not the case as of right now.

### Exercise 7

Copy the previous code. Inside `initial_split()`, set `prop` to `3/4`.

```{r a-picture-is-worth-a-7, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r a-picture-is-worth-a-7-hint-1, eval = FALSE}
initial_split(beans, strata = class, prop = ... / ...)
```

```{r include = FALSE}
initial_split(beans, strata = class, prop = 3/4)
```

### 

As you can see, the data has successfully been split with the correct proportion (75% training and 25% testing).

### Exercise 8

Copy the previous code and assign it to a new variable named `bean_split`.

```{r a-picture-is-worth-a-8, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r a-picture-is-worth-a-8-hint-1, eval = FALSE}
... <- initial_split(beans, strata = class, prop = 3/4)
```

```{r include = FALSE}
bean_split <- initial_split(beans, strata = class, prop = 3/4)
```

### 

Another methods for quantifying shapes of objects include perimeter: the perimeter can be measured using the number of pixels in the boundary as well as the area of the bounding box (the smallest rectangle enclosing an object).

### Exercise 9

Now, let's extract the training and testing data. In the code chunk below, type in `training()`, passing in `bean_split`.

```{r a-picture-is-worth-a-9, exercise = TRUE}

```

```{r a-picture-is-worth-a-9-hint-1, eval = FALSE}
training(...)
```

```{r include = FALSE}
training(bean_split)
```

### 

As a reminder, `training()` is used to extract the training data from the data split. As you can see from the output, the training data contains 10,206 rows.

### Exercise 10

Copy the previous code and assign it to a new variable named `bean_train`.

```{r a-picture-is-worth-a-10, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r a-picture-is-worth-a-10-hint-1, eval = FALSE}
... <- training(bean_split)
```

```{r include = FALSE}
bean_train <- training(bean_split)
```

### 

The *major axis* quantifies the longest line connecting the most extreme parts of the object. The *minor axis* is perpendicular to the major axis.

### Exercise 11

Now, let's extract the testing data. In the code chunk below, type in `testing()` and pass in `bean_split`.

```{r a-picture-is-worth-a-11, exercise = TRUE}

```

```{r a-picture-is-worth-a-11-hint-1, eval = FALSE}
testing(...)
```

```{r include = FALSE}
testing(bean_split)
```

### 

Just like `training()`, `testing()` is used to extract the testing data from the data split. As you can see from the output, the training data contains 3,404 rows.

### Exercise 12

Copy the previous code and assign it to a new variable named `bean_test`.

```{r a-picture-is-worth-a-12, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r a-picture-is-worth-a-12-hint-1, eval = FALSE}
... <- testing(bean_split)
```

```{r include = FALSE}
bean_test <- testing(bean_split)
```

### 

The compactness of an object can be measured using the ratio of the object’s area to the area of a circle with the same perimeter. For example, the symbols “•” and “×” have very different compactness.

### Exercise 13

Type in `set.seed()` and pass in `1602`. 

```{r a-picture-is-worth-a-13, exercise = TRUE}

```

```{r a-picture-is-worth-a-13-hint-1, eval = FALSE}
set.seed(...)
```

```{r include = FALSE}
set.seed(1602)
```

### 

There are also different measures of how *elongated* or oblong an object is. For example, the *eccentricity* statistic is the ratio of the major and minor axes. There are also related estimates for roundness and convexity.

### Exercise 14

Now, lets create a validation set of `bean_train`. In the code chunk below, type in `validation_split()` and type in `bean_train`. Also, set `strata` to `class`.

```{r a-picture-is-worth-a-14, exercise = TRUE}

```

```{r a-picture-is-worth-a-14-hint-1, eval = FALSE}
validation_split(..., strata = ...)
```

```{r include = FALSE}
validation_split(bean_train, strata = class)
```

### 

Notice the eccentricity for the different shapes in the image below:

```{r}
knitr::include_graphics("images/pic1.png")
```

Shapes such as circles and squares have low eccentricity while oblong shapes have high values. Also, the metric is unaffected by the rotation of the object.

### Exercise 15

Copy the previous code. Inside `validation_split()`, set `prop` to `4/5`.

```{r a-picture-is-worth-a-15, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r a-picture-is-worth-a-15-hint-1, eval = FALSE}
validation_split(bean_train, strata = class, prop = .../...)
```

```{r include = FALSE}
validation_split(bean_train, strata = class, prop = 3/4)
```

### 

Looking at the images from the previous exercise, many of them features have high correlations; objects with large areas are more likely to have large perimeters. There are often multiple methods to quantify the same underlying characteristics (e.g., size).

### Exercise 16

Copy the previous code and assign it to a new variable named `bean_val`.

```{r a-picture-is-worth-a-16, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r a-picture-is-worth-a-16-hint-1, eval = FALSE}
... <- validation_split(bean_train, strata = class, prop = 3/4)
```

```{r include = FALSE}
bean_val <- validation_split(bean_train, strata = class, prop = 3/4)
```

### 

As a reminder, a validation split takes a single random sample (without replacement) of the original data set to be used for analysis. This sample is then used to evaluate a model's performance and can also be used to tune hyperparameters.

### Exercise 17

Looking at the output of `bean_val`, you can see that there is 1 row, which contains a list. Lets use sub-setting to see the contents of the list.

In the code chunk below, type in `bean_val$splits[[]]`. Inside the double brackets, type in `1`.

```{r a-picture-is-worth-a-17, exercise = TRUE}

```

```{r a-picture-is-worth-a-17-hint-1, eval = FALSE}
bean_val$splits[[...]]
```

```{r include = FALSE}
bean_val$splits[[1]]
```

### 

The double bracket operator, `[[`, and dollar sign, `$`, can be used to extract columns out of a data frame. `[[` can access by position *or* by name, and `$` is specialized for access *by name*.

To visually assess how well different methods perform, the methods on the training set (n = 8163 beans) can be estimated and the results using the validation set (n = 2043) can be displayed.

### Exercise 18

Before beginning any dimensionality reduction, let's spend some time investigating the data. Since it's now known that many of these shape features are probably measuring similar concepts, let’s take a look at the correlation structure of the data.

Load the **corrplot** library using `library()`.

```{r a-picture-is-worth-a-18, exercise = TRUE}

```

```{r a-picture-is-worth-a-18-hint-1, eval = FALSE}
library(...)
```

```{r include = FALSE}
library(corrplot)
```

### 

**corrplot** is a graphical display of a correlation matrix and confidence intervals.

### Exercise 19

In the code chunk below, type in `colorRampPalette()`. Inside this function, type in `c("#91CBD765", "#CA225E")`.

```{r a-picture-is-worth-a-19, exercise = TRUE}

```

```{r a-picture-is-worth-a-19-hint-1, eval = FALSE}
colorRampPalette(c("...", "..."))
```

```{r include = FALSE}
colorRampPalette(c("#91CBD765", "#CA225E"))
```

### 

`colorRampPalette()` is a function that interpolates a set of given colors to create new color palettes and color ramps. The strings that were passed in are color codes represented as hexadecimal values.

### Exercise 20

Copy the previous code and assign it to a new variable named `tmwr_cols`.

```{r a-picture-is-worth-a-20, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r a-picture-is-worth-a-20-hint-1, eval = FALSE}
... <- colorRampPalette(c("#91CBD765", "#CA225E"))
```

```{r include = FALSE}
tmwr_cols <- colorRampPalette(c("#91CBD765", "#CA225E"))
```

### 

In the bean data, 16 morphology features were computed: area, perimeter, major axis length, minor axis length, aspect ratio, eccentricity, convex area, equiv diameter, extent, solidity, roundness, compactness, shape factor 1, shape factor 2, shape factor 3, and shape factor 4.

### Exercise 21

Now, lets create a visual of the correlation matrix of the predictors. Start by piping `bean_train` to `select()`. Inside this function, type in `-class`.

```{r a-picture-is-worth-a-21, exercise = TRUE}

```

```{r a-picture-is-worth-a-21-hint-1, eval = FALSE}
bean_train |>
  select(...)
```

```{r include = FALSE}
bean_train |>
  select(-class)
```

### 

It is important to maintain good data discipline when evaluating dimensionality reduction techniques, especially if you will use them within a model.

### Exercise 22

Copy the previous code and pipe it to `cor()`. 

```{r a-picture-is-worth-a-22, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r a-picture-is-worth-a-22-hint-1, eval = FALSE}
bean_train |>
  select(-class) |>
  ...()
```

```{r include = FALSE}
bean_train |>
  select(-class) |>
  cor()
```

### 

`cor()` is a function that computes the correlation coefficient between numeric variables in a dataset. 

### Exercise 23

Copy the previous code and pipe it to `corrplot()`. Inside this function, set `col` to `tmwr_cols(200)` and `tl.col` to `"black"`.

```{r a-picture-is-worth-a-23, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r a-picture-is-worth-a-23-hint-1, eval = FALSE}
bean_train |>
  select(-class) |>
  cor() |>
  ...(col = tmwr_cols(...), tl.col = "...")
```

```{r include = FALSE}
bean_train |>
  select(-class) |>
  cor() |>
  corrplot(col = tmwr_cols(200), tl.col = "black")
```

### 

This visualization is the correlation matrix of the predictors with variables ordered via clustering. The legend on the right side represents the level of correlation. As you can see, many of these predictors are highly correlated, such as area and perimeter or shape factors 2 and 3.

While it isn't done here,  it is also important to see if this correlation structure significantly changes across the outcome categories. This can help create better models.

### 

Congrats! You have analyzed the `beans` data set, creating various splits and a visual of the correlation matrix of the predictors.

## A Starter Recipe
### 

It’s time to look at the beans data in a smaller space. Let's start with a basic recipe to preprocess the data prior to any dimensionality reduction steps. Several predictors are ratios and so are likely to have skewed distributions. Such distributions can wreak havoc on variance calculations (such as the ones used in PCA).

### Exercise 1

To start, load the **bestNormalize** library using `library()`.

```{r a-starter-recipe-1, exercise = TRUE}

```

```{r a-starter-recipe-1-hint-1, eval = FALSE}
library(...)
```

```{r include = FALSE}
library(bestNormalize)
```

### 

Dimensionality reduction can be used either in feature engineering or in exploratory data analysis. For example, in high-dimensional biology experiments, one of the first tasks, before any modeling, is to determine if there are any unwanted trends in the data (e.g., effects not related to the question of interest, such as lab-to-lab differences).

### Exercise 2

Let's use the training data from the `bean_val` split object to create a recipe. In the code chunk below, type in `recipe()`. Inside this function, type in `class ~ .` and set `data` to `analysis(bean_val$splits[[1]])`.

```{r a-starter-recipe-2, exercise = TRUE}

```

```{r a-starter-recipe-2-hint-1, eval = FALSE}
...(class ~ ., data = ...(bean_val$splits[[...]])) 
```

```{r include = FALSE}
recipe(class ~ ., data = analysis(bean_val$splits[[1]])) 
```

### 

Debugging the data is difficult when there are hundreds of thousands of dimensions, and dimensionality reduction can be an aid for exploratory data analysis.

### Exercise 3

Copy the previous code and pipe it to `step_zv()`. Inside this function, type in `all_numeric_predictors()`.

```{r a-starter-recipe-3, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r a-starter-recipe-3-hint-1, eval = FALSE}
recipe(class ~ ., data = analysis(bean_val$splits[[1]])) |>
  step_zv(...())
```

```{r include = FALSE}
recipe(class ~ ., data = analysis(bean_val$splits[[1]])) |>
  step_zv(all_numeric_predictors())
```

### 

`step_zv()` creates a specification of a recipe step that will remove variables that contain only a single value.

### Exercise 4

Copy the previous code and pipe it to `step_orderNorm()`. Inside this function, type in `all_numeric_predictors()`.

```{r a-starter-recipe-4, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r a-starter-recipe-4-hint-1, eval = FALSE}
recipe(class ~ ., data = analysis(bean_val$splits[[1]])) |>
  step_zv(all_numeric_predictors()) |>
  step_orderNorm(...())
```

```{r include = FALSE}
recipe(class ~ ., data = analysis(bean_val$splits[[1]])) |>
  step_zv(all_numeric_predictors()) |>
  step_orderNorm(all_numeric_predictors())
```

### 

`step_orderNorm()` is a function that creates a specification of a recipe step that will transform data using the ORQ (orderNorm) transformation, which approximates the "true" normalizing transformation if one exists.

### Exercise 5

Copy the previous code and pipe it to `step_normalize()`. Inside this function, type `all_numeric_predictors()`.

```{r a-starter-recipe-5, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r a-starter-recipe-5-hint-1, eval = FALSE}
recipe(class ~ ., data = analysis(bean_val$splits[[1]])) |>
  step_zv(all_numeric_predictors()) |>
  step_orderNorm(all_numeric_predictors()) |>
  step_normalize(...())
```

```{r include = FALSE}
recipe(class ~ ., data = analysis(bean_val$splits[[1]])) |>
  step_zv(all_numeric_predictors()) |>
  step_orderNorm(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors())
```

### 

`step_normalize()` creates a *specification* of a recipe step that will normalize numeric data to have a standard deviation of one and a mean of zero.

### Exercise 6

Copy the previous code and assign it to a new variable named `bean_rec`.

```{r a-starter-recipe-6, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r a-starter-recipe-6-hint-1, eval = FALSE}
... <- 
  recipe(class ~ ., data = analysis(bean_val$splits[[1]])) |>
  step_zv(all_numeric_predictors()) |>
  step_orderNorm(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors())
```

```{r include = FALSE}
bean_rec <- 
  recipe(class ~ ., data = analysis(bean_val$splits[[1]])) |>
  step_zv(all_numeric_predictors()) |>
  step_orderNorm(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors())
```

### 

Remember that when invoking the `recipe()` function, the steps are not estimated or executed in any way.

### 

This recipe will be extended with additional steps for the dimensionality reduction analyses. Before doing so, let’s go over how a recipe can be used outside of a workflow.


## Recipes In The Wild
### 

A workflow containing a recipe uses `fit()` to estimate the recipe and model, then `predict()` to process the data and make model predictions. There are analogous functions in the recipes package that can be used for the same purpose:

- `prep(recipe, training)` fits the recipe to the training set.
- `bake(recipe, new_data)` applies the recipe operations to `new_data`

### Exercise 1

Let’s estimate `bean_rec` using the training set data, with `prep()`. In the code chunk below, type in `prep()` and pass in `bean_rec`.

```{r recipes-in-the-wild-1, exercise = TRUE}

```

```{r recipes-in-the-wild-1-hint-1, eval = FALSE}
prep(...)
```

```{r include = FALSE}
prep(bean_rec)
```

### 

Note in the output that the steps have been trained and that the selectors are no longer general (i.e., `all_numeric_predictors()`); they now show the actual columns that were selected. 

Also, `prep(bean_rec)` does not require the `training` argument. You can pass any data into that argument, but omitting it means that the original `data` from the call to `recipe()` will be used. In our case, this was the training set data.

### Exercise 2

Copy the previous code and assign it to a new variable named `bean_rec_trained`.

```{r recipes-in-the-wild-2, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r recipes-in-the-wild-2-hint-1, eval = FALSE}
... <- prep(bean_rec)
```

```{r include = FALSE}
bean_rec_trained <- prep(bean_rec)
```

### 

Remember that `prep()` for a recipe is like `fit()` for a model.

### Exercise 3

Once new steps are added to this estimated recipe, reapplying `prep()` will estimate only the untrained steps. This will come in handy when different feature extraction methods are tried.

Here is an example of this process. Press "Run code" (Note: This will throw an error, as `cornbread` is not a real predictor. The code below is just an example of adding a step and reapplying `prep()`).

```{r recipes-in-the-wild-3, exercise = TRUE}
bean_rec_trained |> 
  step_dummy(cornbread) |>  # <- not a real predictor
  prep(verbose = TRUE)
```

```{r include = FALSE}
#bean_rec_trained |> 
#  step_dummy(cornbread) |>  # <- not a real predictor
#  prep(verbose = TRUE)
```

### 

In geenral, if you encounter errors when working with a recipe, `prep()` can be used with its `verbose` option to troubleshoot.

### Exercise 4

Another option that can help you understand what happens in the analysis is `log_changes`. In the code chunk below, pipe `bean_rec` to `prep()`. Inside this function, set `log_changes` to `TRUE`.

```{r recipes-in-the-wild-4, exercise = TRUE}

```

```{r recipes-in-the-wild-4-hint-1, eval = FALSE}
bean_rec |> 
  prep(... = ...)
```

```{r include = FALSE}
bean_rec |> 
  prep(log_changes = TRUE)
```

### 

Since `prep()` has to execute the recipe as it proceeds, it may be advantageous to keep this version of the training set so that, if that data set is to be used later, redundant calculations can be avoided. However, if the training set is big, it may be problematic to keep such a large amount of data in memory. Use `retain = FALSE` to avoid this.

### Exercise 5

Using `bake()` with a recipe is much like using `predict()` with a model; the operations estimated from the training set are applied to any data, like testing data or new data at prediction time.

In the code chunk below, pipe `bean_val$splits` to `pluck()`. Inside this function, type in `1`. 

```{r recipes-in-the-wild-5, exercise = TRUE}

```

```{r recipes-in-the-wild-5-hint-1, eval = FALSE}
bean_val$splits |>
  ...(1)
```

```{r include = FALSE}
bean_val$splits |>
  pluck(1)
```

### 

`pluck()` implements a generalized form of the double bracket operator, `[[`, that allow you to index deeply and flexibly into data structures.

### Exercise 6

Copy the previous code and pipe it to `assessment()`. Then, press "Run code".

```{r recipes-in-the-wild-6, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r recipes-in-the-wild-6-hint-1, eval = FALSE}
bean_val$splits |>
  pluck(1) |>
  ...()
```

```{r include = FALSE}
bean_val$splits |>
  pluck(1) |>
  assessment()
```

### 

In this scenario, `assessment()` returns the code as a data frame (specifically a tibble).

### Exercise 7

Copy the previous code and assign it to a new variable named `bean_validation`.

```{r recipes-in-the-wild-7, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r recipes-in-the-wild-7-hint-1, eval = FALSE}
... <- 
  bean_val$splits |>
  pluck(1) |>
  assessment()
```

```{r include = FALSE}
bean_validation <- 
  bean_val$splits |>
  pluck(1) |>
  assessment()
```

### 

This [link](https://recipes.tidymodels.org/reference/bake.html) contains more information about `bake()`.

### Exercise 8

Now, lets process `bean_validation` with `bake()`. In the code chunk below, type in `bake()`. Inside this funciton, type in `bean_rec_trained` and set `new_data` to `bean_validation`.

```{r recipes-in-the-wild-8, exercise = TRUE}

```

```{r recipes-in-the-wild-8-hint-1, eval = FALSE}
bake(..., new_data = ...)
```

```{r include = FALSE}
bake(bean_rec_trained, new_data = bean_validation)
```

### 

By using `bake()`, the validation set samples have now been processed.

### Exercise 9

Copy the previous code and assign it to a new variable named `bean_val_processed`.

```{r recipes-in-the-wild-9, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r recipes-in-the-wild-9-hint-1, eval = FALSE}
... <- bake(bean_rec_trained, new_data = bean_validation)
```

```{r include = FALSE}
bean_val_processed <- bake(bean_rec_trained, new_data = bean_validation)
```

### 

Another potential consequence of having a multitude of predictors is possible harm to a model. The simplest example is a method like ordinary linear regression where the number of predictors should be less than the number of data points used to fit the model. 

### Exercise 10

Now, lets create 2 histograms of the `area` predictor that show the before and after of the prepared recipe. When completed, the histograms will look like this:

```{r}
p1
p2
```

In the code chunk below, pipe `bean_validation` to `ggplot()`. Inside this function, using `aes()`, set `x` to `area`.

```{r recipes-in-the-wild-10, exercise = TRUE}

```

```{r recipes-in-the-wild-10-hint-1, eval = FALSE}
bean_validation |> 
  ...(aes(x = ...))
```

```{r include = FALSE}
bean_validation |> 
  ggplot(aes(x = area))
```

### 

Another potential consequence of having a multitude of predictors is multicollinearity, where between-predictor correlations can negatively impact the mathematical operations used to estimate a model.

### Exercise 11

Copy the previous code and add `geom_histogram()`. Inside this function, set `bins` to `30` and `alpha` to `1/3`. 

```{r recipes-in-the-wild-11, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r recipes-in-the-wild-11-hint-1, eval = FALSE}
bean_validation |> 
  ggplot(aes(x = area)) +
  geom_histogram(bins = ..., alpha = ...)
```

```{r include = FALSE}
bean_validation |> 
  ggplot(aes(x = area)) +
  geom_histogram(bins = 30, alpha = 1/3)
```

### 

If there are an extremely large number of predictors, it is fairly unlikely that there are an equal number of real underlying effects. Predictors may be measuring the same latent effect(s), and thus such predictors will be highly correlated. Many dimensionality reduction techniques thrive in this situation. In fact, most can be effective only when there are such relationships between predictors that can be exploited.

### Exercise 12

Copy the previous code. Inside `geom_histogram()`, set `color` to `"white"` and `fill` to `"blue"`.

```{r recipes-in-the-wild-12, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r recipes-in-the-wild-12-hint-1, eval = FALSE}
bean_validation |> 
  ggplot(aes(x = area)) +
  geom_histogram(bins = 30, alpha = 1/3, color = "...", fill = "...")
```

```{r include = FALSE}
bean_validation |> 
  ggplot(aes(x = area)) +
  geom_histogram(bins = 30, alpha = 1/3, color = "white", fill = "blue")
```

### 

When starting a new modeling project, reducing the dimensions of the data may provide some intuition about how hard the modeling problem may be.

### Exercise 13

Copy the previous code and add `ggtitle()`. Inside this function, type in `"Original validation set data"`. The final graph should look like this:

```{r}
p1
```

```{r recipes-in-the-wild-13, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r recipes-in-the-wild-13-hint-1, eval = FALSE}
bean_validation |> 
  ggplot(aes(x = area)) +
  geom_histogram(bins = 30, alpha = 1/3, color = "white", fill = "blue") +
  ggtitle("...")
```

```{r include = FALSE}
bean_validation |> 
  ggplot(aes(x = area)) +
  geom_histogram(bins = 30, alpha = 1/3, color = "white", fill = "blue") +
  ggtitle("Original validation set data")
```

### 

`ggtitle()` is another way to add a title to a graph. `labs()` also has the capibility of doing this, but it's good to know other ways as well. 

### Exercise 14

Now, lets create the processed validation set graph, which will look like this:

```{r}
p2
```

Pipe `bean_val_processed` to `ggplot()`. Inside this function, using `aes()`, set `x` to `area`.

```{r recipes-in-the-wild-14, exercise = TRUE}

```

```{r recipes-in-the-wild-14-hint-1, eval = FALSE}
bean_val_processed |>
  ...(aes(x = ...))
```

```{r include = FALSE}
bean_val_processed |>
  ggplot(aes(x = area))
```

### 

Principal component analysis (PCA) is one of the most straightforward methods for reducing the number of columns in the data set because it relies on linear methods and is unsupervised (i.e., does not consider the outcome data).

### Exercise 15

Copy the previous code and add `geom_histogram()`. Inside this function, set `bins` to `30`, `alpha` to `1/3`, `color` to `"white"`, and `fill` to `"red"`.

```{r recipes-in-the-wild-15, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r recipes-in-the-wild-15-hint-1, eval = FALSE}
bean_val_processed |>
  ggplot(aes(x = area)) +
  geom_histogram(bins = ..., alpha = ..., color = "...", fill = "...")
```

```{r include = FALSE}
bean_val_processed |>
  ggplot(aes(x = area)) +
  geom_histogram(bins = 30, alpha = 1/3, color = "white", fill = "red")
```

### 

For a high-dimensional classification problem, an initial plot of the main PCA components might show a clear separation between the classes. If this is the case, then it is fairly safe to assume that a linear classifier might do a good job. However, the converse is not true; a lack of separation does not mean that the problem is insurmountable.

### Exercise 16

Copy the previous code and add `ggtitle()`. Inside this function, type `"Processed validation set data"`. The final graph should look like this:

```{r}
p2
```

```{r recipes-in-the-wild-16, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r recipes-in-the-wild-16-hint-1, eval = FALSE}
bean_val_processed |>
  ggplot(aes(x = area)) +
  geom_histogram(bins = 30, alpha = 1/3, color = "white", fill = "red") +
  ggtitle("...")
```

```{r include = FALSE}
bean_val_processed |>
  ggplot(aes(x = area)) +
  geom_histogram(bins = 30, alpha = 1/3, color = "white", fill = "red") +
  ggtitle("Processed validation set data")
```

### 

As you can see, the processed validation set data is much more different than the original validation set data.

### Exercise 17

As previously mentioned, using `prep(recipe, retain = TRUE)` keeps the existing processed version of the training set in the recipe. This enables the user to use `bake(recipe, new_data = NULL)`, which returns that data set without further computations.

Press "Run code".

```{r recipes-in-the-wild-17, exercise = TRUE}
bake(bean_rec_trained, new_data = NULL) |> nrow()

bean_val$splits |> pluck(1) |> analysis() |> nrow()
```

```{r include = FALSE}
bake(bean_rec_trained, new_data = NULL) |> nrow()

bean_val$splits |> pluck(1) |> analysis() |> nrow()
```

### 

If the training set is not pathologically large, using this value of `retain` can save a lot of computational time.

Also, additional selectors can be used in the call to specify which columns to return. The default selector is `everything()`, but more specific directives can be used.

### 

Congrats! You have learned how to bake a recipe with the use of `bake()`. 


## Feature Extraction Techniques
### 

Since recipes are the primary option in tidymodels for dimensionality reduction, let’s write a function that will estimate the transformation and plot the resulting data in a scatter plot matrix via the **ggforce** package.

### Exercise 1

Load the **ggforce** package using `library()`.

```{r feature-extraction-t-1, exercise = TRUE}

```

```{r feature-extraction-t-1-hint-1, eval = FALSE}
library(...)
```

```{r include = FALSE}
library(ggforce)
```

### 

**ggforce** is an accelerated version of the **ggplot2** package.

### Exercise 2

Now, lets start creating the function. In the code chunk below, type in `function(){}`. Inside the parenthesis, type in `recipe` and set `data` to `assessment(bean_val$splits[[1]])`.

```{r feature-extraction-t-2, exercise = TRUE}

```

```{r feature-extraction-t-2-hint-1, eval = FALSE}
function(..., dat = ...(bean_val$splits[[...]])) {
  
}
```

```{r include = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  
}
```

### 

Methods such as PCA represent the original predictors using a smaller subset of new features. All of the original predictors are required to compute these new features. The exception to this are sparse methods that have the ability to completely remove the impact of predictors when creating the new features.

### Exercise 3

Copy the previous code. Inside the function, pipe `recipe` to `prep()`.

```{r feature-extraction-t-3, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r feature-extraction-t-3-hint-1, eval = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    ...()
}
```

```{r include = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep()
}
```

### 

[Chapter 19](https://r4ds.had.co.nz/functions.html#choosing-names) of the [*R for Data Science*](https://r4ds.hadley.nz/) textbook provides information about functions `function()`.

### Exercise 4

Copy the previous code. Pipe `prep()` to `bake()`. Inside `bake()`, set `new_data` to `dat`.

```{r feature-extraction-t-4, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r feature-extraction-t-4-hint-1, eval = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    ...(new_data = dat)
}
```

```{r include = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    bake(new_data = dat)
}
```

### 

The `new_data` argument is a data frame or tibble for whom the preprocessing will be applied. If `NULL` is given to `new_data`, the pre-processed training data will be returned (assuming that `prep(retain = TRUE)` was used).

### Exercise 5

Copy the previous code and pipe `bake()` to `ggplot()`. Inside this function, using `aes()`, set `x` to `.panel_x`, `y` to `.panel_y`, `color` to `class`, and `fill` to `class`.

```{r feature-extraction-t-5, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r feature-extraction-t-5-hint-1, eval = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    bake(new_data = dat) |>
    ggplot(...(x = ..., y = ..., color = class, fill = class))
}
```

```{r include = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    bake(new_data = dat) |>
    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class))
}
```

### 

Another type of dimensionality reduction tecnique is t-Distributed Stochastic Neighbor Embedding, or t-SNE. t-SNE is particularly useful for reducing high-dimensional data to two or three dimensions, which can then be visualized.

### Exercise 6

Copy the previous code and add `geom_point()` to the graph. Inside `geom_point()`, set `alpha` to `0.4` and `size` to `0.5`. 

```{r feature-extraction-t-6, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r feature-extraction-t-6-hint-1, eval = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    bake(new_data = dat) |>
    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +
    geom_point(alpha = ..., size = ...)
}
```

```{r include = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    bake(new_data = dat) |>
    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +
    geom_point(alpha = 0.4, size = 0.5)
}
```

### 

As a reminder, the `alpha` argument inside `geom_point()` controls the transparency or opacity of the points.

### Exercise 7

Copy the previous code and add `geom_autodensity()` to the plot. Inside this function, set `alpha` to `0.3`.

```{r feature-extraction-t-7, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r feature-extraction-t-7-hint-1, eval = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    bake(new_data = dat) |>
    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +
    geom_point(alpha = 0.4, size = 0.5) +
    geom_autodensity(alpha = ...)
}
```

```{r include = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    bake(new_data = dat) |>
    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +
    geom_point(alpha = 0.4, size = 0.5) +
    geom_autodensity(alpha = 0.3)
}
```

### 

`geom_autodensity()` is a distribution geoms that fills the panel and works with discrete and continuous data.

### Exercise 8

Copy the previous code and add `facet_matrix()` to the graph. Inside `facet_matrix()`, type `vars(-class)` and set `layer.diag` to `2`.

```{r feature-extraction-t-8, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r feature-extraction-t-8-hint-1, eval = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    bake(new_data = dat) |>
    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +
    geom_point(alpha = 0.4, size = 0.5) +
    geom_autodensity(alpha = 0.3) +
    facet_matrix(...(-class), layer.diag = ...)
}
```

```{r include = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    bake(new_data = dat) |>
    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +
    geom_point(alpha = 0.4, size = 0.5) +
    geom_autodensity(alpha = 0.3) +
    facet_matrix(vars(-class), layer.diag = 2)
}
```

### 

`facet_matrix()` allows you to put different data columns into different rows and columns in a grid of panels.

### Exercise 9

Copy the previous code and add `scale_color_brewer()` to the graph. Inside this function, set `palette` to `"Dark2"`.

```{r feature-extraction-t-9, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r feature-extraction-t-9-hint-1, eval = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    bake(new_data = dat) |>
    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +
    geom_point(alpha = 0.4, size = 0.5) +
    geom_autodensity(alpha = 0.3) +
    facet_matrix(vars(-class), layer.diag = 2) + 
    scale_color_brewer(palette = "...")
}
```

```{r include = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    bake(new_data = dat) |>
    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +
    geom_point(alpha = 0.4, size = 0.5) +
    geom_autodensity(alpha = 0.3) +
    facet_matrix(vars(-class), layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2")
}
```

### 

As a reminder, `scale_color_brewer()` is a function that provides sequential, diverging and qualitative colour schemes.

### Exercise 10

Copy the previous code and add `scale_fill_brewer()` to the graph. Inside this function, set `palette` to `"Dark2"`.

```{r feature-extraction-t-10, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r feature-extraction-t-10-hint-1, eval = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    bake(new_data = dat) |>
    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +
    geom_point(alpha = 0.4, size = 0.5) +
    geom_autodensity(alpha = 0.3) +
    facet_matrix(vars(-class), layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2") + 
    scale_fill_brewer(palette = "...")
}
```

```{r include = FALSE}
function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    bake(new_data = dat) |>
    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +
    geom_point(alpha = 0.4, size = 0.5) +
    geom_autodensity(alpha = 0.3) +
    facet_matrix(vars(-class), layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2") + 
    scale_fill_brewer(palette = "Dark2")
}
```

### 

`scale_fill_brewer()` fills in the data on the graph with a certain color (in this case it is `"Dark2"`).

### Exercise 11

Finally, copy the previous code and assign the entire function to a new variable named `plot_validation_results`.

```{r feature-extraction-t-11, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r feature-extraction-t-11-hint-1, eval = FALSE}
... <- function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    bake(new_data = dat) |>
    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +
    geom_point(alpha = 0.4, size = 0.5) +
    geom_autodensity(alpha = 0.3) +
    facet_matrix(vars(-class), layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2") + 
    scale_fill_brewer(palette = "Dark2")
}
```

```{r include = FALSE}
plot_validation_results <- function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe |>
    prep() |>
    bake(new_data = dat) |>
    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +
    geom_point(alpha = 0.4, size = 0.5) +
    geom_autodensity(alpha = 0.3) +
    facet_matrix(vars(-class), layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2") + 
    scale_fill_brewer(palette = "Dark2")
}
```

### 

This function will be reused several times in this tutorial.

### Exercise 12

PCA has been mentioned several times already in this book, and it’s time to go into more detail. PCA is an unsupervised method that uses linear combinations of the predictors to define new features. These features attempt to account for as much variation as possible in the original data.

Let's add `step_pca()` to the original recipe and use the function to visualize the results on the validation set. In the code chunk below, pipe `bean_rec_trained` to `step_pca()`. Inside this function, type in `all_numeric_predictors()` and set `num_comp` to `4`.

```{r feature-extraction-t-12, exercise = TRUE}

```

```{r feature-extraction-t-12-hint-1, eval = FALSE}
bean_rec_trained |>
  step_pca(...(), num_comp = ...)
```

```{r include = FALSE}
bean_rec_trained |>
  step_pca(all_numeric_predictors(), num_comp = 4)
```

### 

`step_pca()` creates a specification of a recipe step that will convert numeric data into one or more principal components.

### Exercise 13

Copy the previous code and pipe it to the function you created, `plot_validation_results()`. Then, pipe that function to `ggtitle("Principal Component Analysis")` (view the hint if you are confused).

```{r feature-extraction-t-13, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r feature-extraction-t-13-hint-1, eval = FALSE}
bean_rec_trained |>
  step_pca(all_numeric_predictors(), num_comp = 4) |>
  ...() |>
  ggtitle("...")
```

```{r include = FALSE}
bean_rec_trained |>
  step_pca(all_numeric_predictors(), num_comp = 4) |>
  plot_validation_results() |>
  ggtitle("Principal Component Analysis")
```

### 

As you can see, the first two components `PC1` and `PC2`, especially when used together, do an effective job distinguishing between or separating the classes. This may lead you to expect that the overall problem of classifying these beans will not be especially difficult.

### Exercise 14

Recall that PCA is unsupervised. For these data, it turns out that the PCA components that explain the most variation in the predictors also happen to be predictive of the classes. What features are driving performance? The **learntidymodels** package has functions that can help visualize the top features for each component.

Load the **learntidymodels** package using `library()`.

```{r feature-extraction-t-14, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r feature-extraction-t-14-hint-1, eval = FALSE}
library(...)
```

```{r include = FALSE}
library(learntidymodels)
```

### 

Click [here](https://github.com/tidymodels/learntidymodels) to learn more about this package.

### Exercise 15

In the code chunk below, pipe `bean_rec_trained` to `step_pca()`. Inside this function, type in `all_numeric_predictors()` and set `num_comp` to `4`.

```{r feature-extraction-t-15, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r feature-extraction-t-15-hint-1, eval = FALSE}
bean_rec_trained |>
  step_pca(...(), num_comp = ...)
```

```{r include = FALSE}
bean_rec_trained |>
  step_pca(all_numeric_predictors(), num_comp = 4)
```

### 

As a reminder, `all_numeric_predictors()` is a function that is used to select variables in a formula that have certain roles.

### Exercise 16

Copy the previous code and pipe it to `prep()`. 

```{r feature-extraction-t-16, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r feature-extraction-t-16-hint-1, eval = FALSE}
bean_rec_trained |>
  step_pca(all_numeric_predictors(), num_comp = 4) |>
  ...()
```

```{r include = FALSE}
bean_rec_trained |>
  step_pca(all_numeric_predictors(), num_comp = 4) |>
  prep()
```

### 

Here is an image that summarizes recipe-related functions:

```{r}
knitr::include_graphics("images/pic2.png")
```

### Exercise 17

Copy the previous code and pipe it to ` plot_top_loadings()`. Inside this function, type in `component_number <= 4` as the first argument and set `n` to `5` as the second argument.

```{r feature-extraction-t-17, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r feature-extraction-t-17-hint-1, eval = FALSE}
bean_rec_trained |>
  step_pca(all_numeric_predictors(), num_comp = 4) |>
  prep() |>
  plot_top_loadings(component_number <= 4, ... = ...)
```

```{r include = FALSE}
bean_rec_trained |>
  step_pca(all_numeric_predictors(), num_comp = 4) |>
  prep() |>
  plot_top_loadings(component_number <= 4, n = 5)
```

### 

`plot_top_loadings()` is a function that produces a plot of the `n` largest component loadings.

### Exercise 18

Copy the previous code and pipe it to `scale_fill_brewer()`. Inside this function, set `palette` to `"Paired"`,

```{r feature-extraction-t-18, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r feature-extraction-t-18-hint-1, eval = FALSE}
bean_rec_trained |>
  step_pca(all_numeric_predictors(), num_comp = 4) |>
  prep() |>
  plot_top_loadings(component_number <= 4, n = 5) |>
  scale_fill_brewer(palette = "...")
```

```{r include = FALSE}
bean_rec_trained |>
  step_pca(all_numeric_predictors(), num_comp = 4) |>
  prep() |>
  plot_top_loadings(component_number <= 4, n = 5) |>
  scale_fill_brewer(palette = "Paired")
```

### 

This [link](https://r-graph-gallery.com/38-rcolorbrewers-palettes.html) provides the various color palettes that the **RColorBrewer** package provides.

### Exercise 19

Copy the previous code and pipe it to `ggtitle()`. Inside this function, pass in `"Principal Component Analysis"`.

```{r feature-extraction-t-19, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r feature-extraction-t-19-hint-1, eval = FALSE}
bean_rec_trained |>
  step_pca(all_numeric_predictors(), num_comp = 4) |>
  prep() |>
  plot_top_loadings(component_number <= 4, n = 5) |>
  scale_fill_brewer(palette = "Paired") |>
  ggtitle("...")
```

```{r include = FALSE}
bean_rec_trained |>
  step_pca(all_numeric_predictors(), num_comp = 4) |>
  prep() |>
  plot_top_loadings(component_number <= 4, n = 5) |>
  scale_fill_brewer(palette = "Paired") |>
  ggtitle("Principal Component Analysis")
```

### 

The top loadings are mostly related to the cluster of correlated predictors shown in the top-left portion of the previous correlation plot: perimeter, area, major axis length, and convex area. These are all related to bean size. 

## Summary
### 

<!-- Two to four sentences which bring the lessons of the tutorial together for the student. What do they know now that they did not know before? How does this tutorial connect to other tutorials? OK if this is very similar to the Introduction. You made a promise as to what they would learn. You (we hope!) kept that promise.-->

```{r download-answers, child = system.file("child_documents/download_answers.Rmd", package = "tutorial.helpers")}
```
